{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e52400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q datasets loralib sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9903d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa57ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced3d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c58b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"example_data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49347cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f345de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = \"<|USER|>{query_str}<|ASSISTANT|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"auto\" # the device to load the model onto\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map=device,\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": torch.float32},\n",
    "\n",
    ")\n",
    "     \n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "#prompt = \"My favourite condiment is\"\n",
    "\n",
    "#model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "#model.to(device)\n",
    "\n",
    "#generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "#tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a854b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embed_model =HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\",model_kwargs = {'device': 'cpu'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27bc1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d540094",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbadf2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"what was the revenuew of Q3 of BAyer?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f839d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
