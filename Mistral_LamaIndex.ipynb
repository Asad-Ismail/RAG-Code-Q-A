{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17412960",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets loralib sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca48c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a65732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b2e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents = SimpleDirectoryReader(\"example_data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e61bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(file_path):\n",
    "    return {\"file_name\": file_path.split(\"/\")[-1]}\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"/home/ec2-user/SageMaker/VegRD_APD_Fruit_Phenotyping/scripts\",recursive=True,\n",
    "                              required_exts=[\".py\"])\n",
    "\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131b00e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e29638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='d837c43e-295f-473d-aba3-e3e04ab1782c', embedding=None, metadata={'file_path': '/home/ec2-user/SageMaker/VegRD_APD_Fruit_Phenotyping/scripts/Hot_Peppers.py', 'file_name': 'Hot_Peppers.py', 'file_type': 'text/x-python', 'file_size': 20914, 'creation_date': '2023-12-20', 'last_modified_date': '2023-12-20', 'last_accessed_date': '2023-12-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='08523b607c9e3668532ce2f5ca673ebb63953a6501347159d0acb7b356d6efca', text='import os\\nimport cv2\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport boto3\\nimport warnings\\nfrom tqdm import tqdm\\nfrom statistics import median\\nfrom skimage.morphology import medial_axis\\nfrom scipy.interpolate import splprep, splev\\nfrom scipy.ndimage import distance_transform_edt\\nfrom skimage.morphology import skeletonize\\nfrom lib.pred_utils import *\\nfrom lib.phenome_utils import *\\nfrom colorlog import ColoredFormatter\\nfrom config import HP_config_defaults as config_defaults\\nimport math\\nimport numpy as np\\nimport logging\\nfrom lib.aggregate import aggregate_data\\nwarnings.filterwarnings(\"ignore\")\\n\\nfrom skimage.morphology import skeletonize\\nfrom dsepruning import skel_pruning_DSE\\nfrom scipy.ndimage import distance_transform_edt\\nfrom lib.curved_utils import *\\n\\nimport argparse\\nimport json\\n# Parse command-line arguments\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\\'--config\\', help=\\'Path to configuration file\\')\\nargs = parser.parse_args()\\n\\n# Config dynamodb\\ndynamodb = boto3.resource(\\'dynamodb\\')\\ntable = dynamodb.Table(\\'veg-vps-Image_Analyzer-Images\\')\\n\\ndef load_config():\\n    return config_defaults\\n\\ndef setup_logging():\\n    logger = logging.getLogger()\\n    logger.setLevel(logging.INFO)\\n    handler = logging.StreamHandler()\\n    handler.setLevel(logging.INFO)\\n    formatter = ColoredFormatter(\\n        \"%(log_color)s%(levelname)-8s%(reset)s %(log_color)s%(message)s%(reset)s\",\\n        datefmt=None,\\n        reset=True,\\n        log_colors={\\n            \\'DEBUG\\': \\'cyan\\',\\n            \\'INFO\\': \\'green\\',\\n            \\'WARNING\\': \\'yellow\\',\\n            \\'ERROR\\': \\'red\\',\\n            \\'CRITICAL\\': \\'red\\',\\n        }\\n    )\\n    handler.setFormatter(formatter)\\n    logger.addHandler(handler)\\n    return logger\\n\\n## Keeping it here instead of curved_utils because of pruning import problem\\ndef backbone(patch, fname=None):\\n    _, patch = cv2.threshold(patch, 10, 255, cv2.THRESH_BINARY)\\n    skeleton = skeletonize(patch.copy()//255)\\n    dist = distance_transform_edt(patch.copy(), return_indices=False, return_distances=True)\\n    thres=[10000,1000,100]\\n    for th in thres:\\n        pruned = skel_pruning_DSE(skeleton, dist, th).astype(np.uint8)*255\\n        # If the pruned is not completely zero break\\n        if len(np.nonzero(pruned)[0]):\\n            break  \\n    points=skeletonEndpoints(pruned.copy())\\n    cv2.circle(pruned,(points[0][1],points[0][0]),2,color=255)\\n    cv2.circle(pruned,(points[1][1],points[1][0]),2,color=255)\\n    extended= extendSkel(points,pruned.copy(),patch.copy()//255)\\n    return extended\\n\\n\\n\\ndef load_images(img_path):\\n    all_imgs = []\\n    fnames = []\\n    for file_index, filename in enumerate(os.listdir(img_path)):\\n        if filename.endswith((\".tar\", \".json\", \".ipynb_checkpoints\")):\\n            continue\\n        f_p = os.path.join(img_path, filename)\\n        fnames.append(filename)\\n        all_imgs.append(f_p)\\n    return all_imgs, fnames\\n\\ndef calculate_scaled_points(points, ppx=None, ppy=None):\\n    if ppx is not None and ppy is not None:\\n        return np.array(points) * np.array([ppx, ppy])\\n    return np.array(points)\\n\\n\\ndef eucledian_distance(points, ppx=None, ppy=None):\\n    scaled_points = calculate_scaled_points(points, ppx, ppy)\\n    differences = np.diff(scaled_points, axis=0)\\n    squared_differences = np.square(differences)\\n    squared_distances = np.sum(squared_differences, axis=1)\\n    length = np.sqrt(squared_distances).sum()\\n    return length\\n\\n\\ndef eucledian_list(inp, ppx=None, ppy=None):\\n    return [eucledian_distance(item, ppx, ppy) for item in inp]\\n\\n\\ndef convert_pixels_to_measure(params, ppx=None, ppy=None):\\n    \"\"\"Convert pixels to real units cm/mm if ppx and ppy are defined, otherwise find lengths of curves\"\"\"\\n    if ppx is not None and ppy is not None:\\n        params[\"area\"] *= ppx * ppy\\n        params[\"perimeter\"] *= ppx\\n    params[\"mid_height_width\"] = eucledian_distance(params[\"mid_height_width_p\"], ppx, ppy)\\n    params[\"max_width\"] = eucledian_distance(params[\"max_width_p\"], ppx, ppy)\\n    params[\"all_widths\"] = eucledian_list(params[\"all_widths_p\"], ppx, ppy)\\n    params[\"mid_width_height\"] = eucledian_distance(params[\"mid_width_height_p\"], ppx, ppy)\\n    params[\"max_height\"] = eucledian_distance(params[\"max_height_p\"], ppx, ppy)\\n    params[\"all_heights\"] = eucledian_list(params[\"all_heights_p\"], ppx, ppy)\\n    if isinstance(params[\"c_length\"],list) or isinstance(params[\"c_length\"],np.ndarray):\\n        params[\"c_length\"] = eucledian_distance(params[\"c_length\"], ppx, ppy)\\n    else:\\n        params[\"c_length\"] *= ppx\\n    return params\\n\\n\\ndef get_unique_xs(per_points):\\n    xs = per_points[:, 0]\\n    u_xs = sorted(set(xs))\\n    return u_xs, xs\\n\\n\\ndef process_segments(u_xs, xs, ys):\\n    top = []\\n    bottom = []\\n    widths = []\\n    for x in u_xs:\\n        segments = min_max_segment(x, xs, ys)\\n        for segment in segments:\\n            top.append(segment[0])\\n            bottom.append(segment[1])\\n            widths.append(segment[2])\\n    return top, bottom, widths\\n\\ndef calculate_shape_metrics(kwargs, ppx=None, ppy=None):\\n    max_height = kwargs[\"max_height\"]\\n    max_width = kwargs[\"max_width\"]\\n    mid_height_width = kwargs[\"mid_height_width\"]\\n    mid_width_height = kwargs[\"mid_width_height\"]\\n    c_length = kwargs[\"c_length\"]\\n    t_block = kwargs[\"t_block\"]\\n    b_block = kwargs[\"b_block\"]\\n    maxheight_to_maxwidth = max_height / max_width\\n    mid_width_height_to_mid_height_width = mid_width_height / mid_height_width\\n    c_length_to_mid_height_width = c_length / mid_height_width\\n    proximal_blockiness = eucledian_distance(t_block, ppx=ppx, ppy=ppy) / mid_height_width\\n    distal_blockiness = eucledian_distance(b_block, ppx=ppx, ppy=ppy) / mid_height_width\\n    fruit_triangle = eucledian_distance(t_block, ppx=ppx, ppy=ppy) / eucledian_distance(b_block, ppx=ppx, ppy=ppy)\\n\\n    return (maxheight_to_maxwidth, mid_width_height_to_mid_height_width, c_length_to_mid_height_width,\\n            proximal_blockiness, distal_blockiness, fruit_triangle)\\n\\n\\ndef phenotype_measurement_pepper(img, patch, fname, fruitIdx, corrupted_color_flag=False, skip_outliers=None, pct_width=0.4, vis=False,ppx=None,ppy=None):\\n    width_quantiles = config_defaults[\"width_quantiles\"]\\n    expected_sample_len = len(width_quantiles)\\n    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\\n    contours = max(contours, key=cv2.contourArea)\\n    per_points = contours.squeeze()\\n    per_points = np.array(sorted(per_points, key=lambda x: x[0]))\\n\\n    perimeter = cv2.arcLength(contours, True)\\n    area = cv2.contourArea(contours)\\n    area_points = np.where(img != 0)\\n    fruit_colors = get_color(patch.copy(), area_points)\\n    \\n    ys = per_points[:, 1]\\n    u_xs,xs = get_unique_xs(per_points)\\n\\n    max_height_p, max_height_v, all_heights_p, all_heights_v = find_max_height_revised(xs, ys)\\n    mid_width_height_p, mid_width_height_v = find_mid_width_height(xs, ys)\\n    \\n    \\n    extended=None\\n    try:\\n    \\n        extended=backbone(img)\\n\\n        ## Widths\\n        top, bottom, widths =get_curved_widths(extended,img)\\n\\n        max_width_p, max_midth_v, all_widths_points, all_widths_p = find_max_width(top, bottom, quantiles=width_quantiles, expected_sample_len=expected_sample_len)\\n\\n        mid_height_width_p, mid_height_width_v = find_mid_height_width(top, bottom)\\n\\n        # curve length\\n        #c_length = find_curve_length(top, bottom, img)\\n        c_length,c_length_p=get_curved_length(extended)\\n        b_block, m_block, t_block = blockiness(top, bottom)\\n        \\n    except Exception as inst:\\n        cv2.imwrite(f\"exception_img.png\",img)\\n        if extended is not None:\\n            cv2.imwrite(f\"Extended_backbone.png\",extended)\\n            \\n        return [None]*22\\n    \\n    \\n    b_block, m_block, t_block = blockiness(top, bottom)\\n\\n    ellipse, ellipse_err = ellipse_fitting_normalized(img)\\n    box, box_aspect = box_fit_normalized(img)\\n\\n    if vis:\\n        phen_img = vis_phenotypes(\\n            per_points,\\n            area_points,\\n            max_height_p,\\n            all_heights_p,\\n            mid_width_height_p,\\n            max_width_p,\\n            all_widths_p,\\n            mid_height_width_p,\\n            c_length_p,\\n            b_block,\\n            m_block,\\n            t_block,\\n            box,\\n            ellipse,\\n            patch.copy(),\\n        )\\n\\n        widths_img = vis_ends_widths(all_widths_p, patch.copy())\\n\\n    else:\\n        phen_img = None\\n        widths_img = None\\n    \\n    params = {\\n    \"area\": area,\\n    \"perimeter\": perimeter,\\n    \"mid_height_width_p\": mid_height_width_p,\\n    \"max_width_p\": max_width_p,\\n    \"all_widths_p\": all_widths_p,\\n    \"mid_width_height_p\": mid_width_height_p,\\n    \"max_height_p\": max_height_p,\\n    \"all_heights_p\": all_heights_p,\\n    \"c_length\": c_length}\\n    \\n    results=convert_pixels_to_measure(params, ppx, ppy)\\n    \\n    results[\"m_block\"]=m_block\\n    results[\"t_block\"]=t_block\\n    results[\"b_block\"]=b_block\\n    \\n    shape_metrics = calculate_shape_metrics(results,ppx=ppx,ppy=ppy)\\n    \\n    maxheight_to_maxwidth, mid_width_height_to_mid_height_width, c_length_to_mid_height_width,proximal_blockiness, distal_blockiness,fruit_triangle=shape_metrics\\n    \\n    return(\\n    results[\"area\"],\\n    results[\"perimeter\"],\\n    results[\"mid_height_width\"],\\n    results[\"max_width\"],\\n    results[\"all_widths\"],\\n    results[\"mid_width_height\"],\\n    results[\"max_height\"],\\n    results[\"all_heights\"],\\n    results[\"c_length\"],\\n    maxheight_to_maxwidth,\\n    mid_width_height_to_mid_height_width,\\n    c_length_to_mid_height_width,\\n    proximal_blockiness,distal_blockiness,fruit_triangle,\\n    ellipse_err,box_aspect,\\n    fruit_colors,\\n    phen_img,\\n    # Added for validation and debugging\\n    max_height_p,\\n    mid_height_width_p,\\n    widths_img)\\n    \\n\\ndef run_inference(det,learn,img):\\n    if img is not None:\\n        detected_cucumber,all_masks,all_patches,boxes,*_=det.pred(img)\\n    else:\\n        print(f\"Invalid Image skipping!!\")\\n        return\\n    cropedPatches=[]\\n    cropedmasks=[]\\n    croppedboxes = []\\n    for i,patch in enumerate(all_patches):\\n        x, y, w, h = cv2.boundingRect(cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY))\\n        newImg = patch[y:y+h, x:x+w]\\n        mask=all_masks[i][y:y+h, x:x+w]\\n        mask = np.where(mask < 50, 0, 255).astype(np.uint8)\\n        cropedPatches.append(newImg)\\n        cropedmasks.append(mask)\\n        croppedboxes.append((x, y, w, h))\\n        \\n    predPoints={}\\n    for i,crop in enumerate(cropedPatches):\\n        img = PILImage.create(crop[...,::-1])\\n        pred=learn.predict(img)\\n        preds=scale_predictions(pred,img.shape)\\n        predPoints[i]=[tuple(preds[0].round().long().numpy()),tuple(preds[1].round().long().numpy())]\\n    \\n    # Correction\\n    corrected_imgs=apply_rotation(cropedPatches,predPoints)\\n    corrected_masks=apply_rotation(cropedmasks,predPoints)\\n\\n\\n    # Use angle of major axis to make the final rotation\\n    ellipse_rotated_imgs = []\\n    ellipse_rotated_masks = []\\n    ellipse_rotated_angle = []\\n    for corrected_img, corrected_mask in zip(corrected_imgs, corrected_masks):\\n        (_,_,_,_,angle),_ = ellipse_fitting_normalized(corrected_mask)\\n        ellipse_rotated_imgs.append(rotate_image(corrected_img, angle-90))\\n        ellipse_rotated_masks.append(rotate_image(corrected_mask, angle-90))\\n        ellipse_rotated_angle.append(angle)\\n    \\n    corrected_imgs = ellipse_rotated_imgs\\n    corrected_masks = ellipse_rotated_masks\\n        \\n    return (\\n        detected_cucumber,\\n        corrected_imgs,\\n        corrected_masks,\\n        croppedboxes,\\n        cropedPatches,\\n        predPoints,\\n        #original boxes are returned to get Gt based on location of boxes\\n        boxes\\n    )\\n\\ndef get_weight(img_name,proj_dynamo):\\n    payload={\\'ProjName\\': proj_dynamo,\\'Name\\': img_name}\\n    response = table.get_item(Key=payload)\\n    try:\\n        weight=float(response[\\'Item\\'][\\'meta_data\\'][\\'weight\\'])\\n        assert weight is not None\\n        # for FTS, assign weight value to 0 if less than 1e-2\\n        if weight < 1e-2:\\n            weight = 0\\n    except Exception as e:\\n        logger.warn(f\"Exception is {e}\")\\n        logger.warn(f\"Response of error is {response} for image {img_name}\")\\n        weight=np.nan\\n    return weight\\n                                                                             \\n\\ndef add_or_append_to_dict(dictionary, key, value):\\n    if key not in dictionary:\\n        dictionary[key] = [value]\\n    else:\\n        dictionary[key].append(value)\\n\\n\\ndef main(config):\\n    logger = setup_logging()\\n    #config = load_config()\\n    # Access configuration values:\\n    imgs_path = config[\\'img_path\\']\\n    proj_dynamo = config[\\'proj_dynamo\\']\\n    save_path = config[\\'save_path\\']\\n    #indiv_results = config[\\'indiv_results\\']\\n    #aggregate_results = config[\\'aggregate_results\\']\\n    os.makedirs(save_path, exist_ok=True)\\n    width_quantiles = config[\\'width_quantiles\\']\\n    expected_sample_len = config[\\'expected_sample_len\\']\\n    ppx = config[\\'ppx\\']\\n    ppy = config[\\'ppy\\']\\n    warnings.filterwarnings(\"ignore\")\\n    # Define models\\' paths\\n    maskrcnn = config[\\'maskrcnn\\']\\n    maskrcnn_config = config[\\'maskrcnn_config\\']\\n    resnet34 = config[\\'resnet34\\']\\n    marketable = config[\\'marketable\\']\\n    plant_traits=config[\\'plant_traits\\']\\n    corrupt_color_check = config[\"filter_corrupt_color\"]\\n    \\n    all_imgs,fnames=load_images(imgs_path)\\n    \\n    # Initialize MASKRCNN\\n    det = detectroninference(maskrcnn_config, maskrcnn, config[\"classes\"], conf_th=config[\"conf_det\"])\\n    # Initialize Resnet34\\n    learn = fast_AILearner(resnet34)\\n    \\n    results={}\\n    globalIndex=0\\n    empty_images=0\\n    skip_fruits=0\\n    plot_names=[]\\n    os.makedirs(save_path,exist_ok=True)\\n    # Set visualization flags\\n    visualize_phenotype = config[\"vis_phen\"]\\n    visualize_detection = config[\"vis_det\"]\\n\\n    logger.info(f\"Visualize detection {visualize_detection},  visualize phenotypes {visualize_phenotype}\")\\n\\n    # Create directory if it does not exist\\n    os.makedirs(save_path, exist_ok=True)\\n    starttime=time.monotonic()\\n\\n    # Process each image\\n    for index,img_path in tqdm(enumerate(all_imgs), total=len(all_imgs)):\\n        \\n        imgStartTime=time.monotonic()\\n        img = cv2.imread(img_path)\\n        detected, corrected_imgs, corrected_masks, cropped_boxes, cropedPatches, predPoints, boxes = run_inference(det,learn,img)\\n        logger.info(f\"Processing file {img_path}\")\\n\\n        # Append plot names to count plot\\n        plot_name = os.path.basename(img_path).split(\"_\")[0]\\n        if plot_name not in plot_names:\\n            plot_names.append(plot_name)\\n        \\n        # do a corrupted color check before phenotyping individual fruits\\n        # set a flag to be used in phenotype_measurement_pepper()\\n        # Kyle C uses L channel > 120\\n        if corrupt_color_check:\\n            corrupted_color_flag = corrupted_color_check(cv2.cvtColor(img.copy(), cv2.COLOR_BGR2Lab)[:,:,0], 120)\\n        else:\\n            corrupted_color_flag=False\\n            \\n        if corrupted_color_flag:\\n            cv2.imwrite(os.path.join(\"wrong_color\",fnames[index]),img)\\n\\n        # Process each fruit in the image\\n        for j, mask in enumerate(corrected_masks):\\n            (\\n                area, perimeter, mid_height_width, max_width, all_widths, mid_width_height,\\n                max_height, all_heights, c_length, maxheight_to_maxwidth,\\n                mid_width_height_to_mid_height_width, c_length_to_mid_height_width,\\n                proximal_blockiness, distal_blockiness, fruit_triangle, ellipse_err,\\n                box_aspect, fruit_colors, phen_img, tmp_pred_ls_ps, tmp_pred_ws_ps, widths_img\\n            ) = phenotype_measurement_pepper (\\n                mask, corrected_imgs[j], os.path.basename(img_path), j,corrupted_color_flag,\\n                skip_outliers=True, vis=visualize_phenotype,ppx=ppx,ppy=ppy)\\n\\n            # If any phenotype is None continue\\n            if area is None:\\n                logger.error(f\"Cannot process this fruit!\")\\n                empty_images += 1\\n                continue\\n            else:\\n                globalIndex+=1\\n\\n            # Save phenotype image\\n            if visualize_phenotype and phen_img is not None:\\n                cv2.imwrite(os.path.join(save_path, f\"Phenotype_{j}_{os.path.basename(img_path)}\"), phen_img)\\n\\n            bb = boxes[j]\\n\\n            keys_and_values = [\\n            (\"Image\", fnames[index]),\\n            (\"Area\", area),\\n            (\"Perimeter\", perimeter),\\n            (\"Mid_Width\", mid_height_width),\\n            (\"Max_Width\", max_width),\\n            (\"FWSD\", np.std(all_widths)),\\n            (\"Mid_Height\", mid_width_height),\\n            (\"Max_Height\", max_height),\\n            (\"FHSD\", float(np.std(all_heights))),\\n            (\"Curved_Height\", c_length),\\n            (\"Maxheight_to_maxwidth\", maxheight_to_maxwidth),\\n            (\"Midheight_to_midwidth\", mid_width_height_to_mid_height_width),\\n            (\"Curveheight_to_midwidth\", c_length_to_mid_height_width),\\n            (\"Proximal_blockiness\", proximal_blockiness),\\n            (\"Distall_blockiness\", distal_blockiness),\\n            (\"Fruit_triangle\", fruit_triangle),\\n            (\"Ellipse\", ellipse_err),\\n            (\"Box\", box_aspect),\\n            (\"Box_x\", (bb[0] + bb[2]) // 2),\\n            (\"Box_y\", (bb[1] + bb[3]) // 2)]\\n\\n            for key, value in keys_and_values:\\n                add_or_append_to_dict(results, key, value)\\n\\n            for i, curr_width in enumerate(all_widths):\\n                add_or_append_to_dict(results, f\"FWID{(i+1):02}\", curr_width)\\n\\n            for i, curr_height in enumerate(all_heights):\\n                add_or_append_to_dict(results, f\"FHT{(i+1):02}\", curr_height)\\n\\n            for color_pheno, color_pheno_tup in config[\"color_pheno\"].items():\\n                if len(color_pheno_tup) == 0:\\n                    add_or_append_to_dict(results, color_pheno, fruit_colors[color_pheno])\\n                else:\\n                    for i, channel in enumerate(color_pheno_tup):\\n                        add_or_append_to_dict(results, f\"{color_pheno}_{channel}\", fruit_colors[color_pheno][i])\\n        imgEndTime=time.monotonic()\\n        logger.info(f\"Image processing time: {imgEndTime - imgStartTime:.2f} seconds\")\\n        # Save detection results\\n        if visualize_detection:\\n            save_results_cv(detected, cropedPatches, predPoints, corrected_imgs, os.path.basename(img_path), os.path.join(save_path, \"inter\"))\\n        #if globalIndex>20:\\n        #    break\\n            \\n    endtime=time.monotonic()\\n    # Print some statistics\\n    logger.warn(f\"Number of empty images: {empty_images}\")\\n    logger.info(f\"Number of images processed: {len(all_imgs) - empty_images}\")\\n    logger.info(f\"Number of unique plots: {len(plot_names)}\")\\n    logger.info(f\"Total processing time: {endtime - starttime:.2f} seconds\")\\n\\n    #with open(\\'results_refactor2.pkl\\', \\'wb\\') as f:\\n    #    pickle.dump(results, f)\\n\\n    pd.set_option(\"display.max_columns\", None)\\n    results_df=pd.DataFrame.from_dict(results,orient=\\'index\\').transpose()\\n    ## Explicitly specify the types of columns\\n    # separate column names by data type\\n    objcols = [\"Image\"]\\n    results_df[results_df.columns.difference(objcols)] = results_df[results_df.columns.difference(objcols)].astype(\\'float64\\')\\n    # Add PLOTBID and Date\\n    results_df.insert(loc=1, column=\\'plot_bid\\', value=results_df[\"Image\"].str.extract(r\\'(.*?)_\\'))\\n    # This is bug in automation code of scalecam but puting a fix here temporarily\\n    #results_df[\\'plot_bid\\'] = results_df[\\'plot_bid\\'].str.replace(r\\'^SHIFT\\', \\'\\')\\n    results_df[\\'plot_bid\\'] = results_df[\\'plot_bid\\'].str.replace(r\\'^.+(?<=SHIFT)\\', \\'\\', regex=True)\\n    results_df.insert(loc=2, column=\\'date\\', value=results_df[\"Image\"].str.extract(r\\'_(.*?)_\\'))\\n    # Sort by date\\n    results_df = results_df.sort_values(\"date\", ascending=True).reset_index(drop=True)\\n    \\n    if not os.path.exists(save_path):\\n        os.makedirs(save_path)\\n    \\n    results_df.to_csv(save_path+\"/indiv.csv\",index=False)\\n    ## Get weight data\\n    images=results_df[\"Image\"].tolist()\\n    weights={\"Image\":[],\"Weights\":[]}\\n    for img in images:\\n        weights[\"Image\"].append(img)\\n        logger.info(f\"Getting weight for image {img}\")\\n        weights[\"Weights\"].append(get_weight(img,proj_dynamo))\\n    weights_df=pd.DataFrame.from_dict(weights,orient=\\'index\\').transpose()\\n    \\n    aggregate_data(results_df, weights_df,imgs_path, save_path, output_filename=\"aggregate.csv\",logger=logger,marketable=marketable,plant_traits=plant_traits)\\n    logger.info(f\"Uploading results in s3 bucket\")\\n    if config[\\'save_aws\\']:\\n        save_aws(directory_path=save_path,bucket_name=\"470086202700-sagemaker\",projname=proj_dynamo)\\n\\nif __name__ == \"__main__\":\\n    from config import HP_config_defaults as config_defaults\\n    # Load configuration from file if provided, otherwise use default config\\n    if args.config:\\n        with open(args.config, \\'r\\') as f:\\n            cfg = json.load(f)\\n    else:\\n        cfg = config_defaults\\n    main(cfg)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72eb3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d9c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = \"<|USER|>{query_str}<|ASSISTANT|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b36075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1df02ba7ea4711b91f2f754fa4b198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"auto\" # the device to load the model onto\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=1000,\n",
    "    generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map=device,\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": torch.float32},\n",
    "\n",
    ")\n",
    "     \n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "#prompt = \"My favourite condiment is\"\n",
    "\n",
    "#model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "#model.to(device)\n",
    "\n",
    "#generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "#tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c4f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embed_model =HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\",model_kwargs = {'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de89fc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-base-en\n",
      "Load pretrained SentenceTransformer: BAAI/bge-base-en\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "embed_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en\",model_kwargs = {'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d592c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install -U angle-emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7808f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOTA Embeddings from huggingface leaderboard\n",
    "\n",
    "from typing import Any, List\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "from llama_index.bridge.pydantic import PrivateAttr\n",
    "\n",
    "## Embedding model specific current SOTA in huggung face leaderboard\n",
    "from angle_emb import AnglE, Prompts\n",
    "\n",
    "\n",
    "class UAEmbeddings(BaseEmbedding):\n",
    "    \n",
    "    _model: Any= PrivateAttr()\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        instructor_model_name: str = 'WhereIsAI/UAE-Large-V1',\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__(**kwargs)  \n",
    "        self._model=AnglE.from_pretrained(instructor_model_name, pooling_strategy='cls',device=\"cpu\")\n",
    "        #self._model.set_prompt(prompt=None)\n",
    "\n",
    "        \n",
    "    def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        # Implementation of the abstract method\n",
    "        embeddings = self._model.encode([ query])\n",
    "        return embeddings[0]\n",
    "        \n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        embeddings = self._model.encode([query])\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        embeddings = self._model.encode([text])\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self._model.encode(\n",
    "            [text for text in texts]\n",
    "        )\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68fb74a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model=UAEmbeddings()\n",
    "embed_model._model.device\n",
    "#embeddings = embed_model._get_text_embedding(\"It is raining cats and dogs here!\")\n",
    "#embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install InstructorEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "from llama_index.bridge.pydantic import PrivateAttr\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "\n",
    "\n",
    "class InstructorEmbeddings(BaseEmbedding):\n",
    "    _model: INSTRUCTOR = PrivateAttr()\n",
    "    _instruction: str = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instructor_model_name: str = \"hkunlp/instructor-large\",\n",
    "        instruction: str = \"Represent code base to easily search from:\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self._model = INSTRUCTOR(instructor_model_name,device=\"cpu\")\n",
    "        self._instruction = instruction\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"instructor\"\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_text_embedding(text)\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        embeddings = self._model.encode([[self._instruction, query]])\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        embeddings = self._model.encode([[self._instruction, text]])\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self._model.encode(\n",
    "            [[self._instruction, text] for text in texts]\n",
    "        )\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model=InstructorEmbeddings()\n",
    "#embeddings = embed_model._get_text_embedding(\"It is raining cats and dogs here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d646532",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "252f4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91f0991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf3b60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dca8205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The curved backbone of hot peppers is calculated using the `get_curved_widths` function in the `phenotype_measurement_pepper` function. This function takes in the extended backbone of the hot pepper and the original image as input and returns the top and bottom of the curved backbone as well as the widths of the curved backbone. The function first calculates the widths of the curved backbone using the `cv2.arcLength` function and then finds the top and bottom of the curved backbone using the `np.where` function. The function then returns the top, bottom, and widths of the curved backbone.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How is curved backbone of hot peppers calculated?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f023fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To turn on/off visualizations of phenotypes, you need to modify the \"visualize\\_phenotype\" variable in the script. If \"visualize\\_phenotype\" is set to True, the phenotype image will be saved and displayed. If it is set to False, the phenotype image will not be saved or displayed.\n",
      "\n",
      "For example, in the script \"Hot\\_Peppers.py\", the \"visualize\\_phenotype\" variable is set as follows:\n",
      "```python\n",
      "visualize_phenotype = True\n",
      "```\n",
      "If you want to turn off the visualization of phenotypes, you can set \"visualize\\_phenotype\" to False:\n",
      "```python\n",
      "visualize_phenotype = False\n",
      "```\n",
      "Similarly, in the script \"multigpu/single\\_hot\\_peppers.py\", the \"visualize\\_phenotype\" variable is set as follows:\n",
      "```python\n",
      "visualize_phenotype = True\n",
      "```\n",
      "If you want to turn off the visualization of phenotypes, you can set \"visualize\\_phenotype\" to False:\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How to turn on/off visulaizations of phenotypes?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06a3345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can turn on/off visualizations of detections and phenotypes of hot peppers by setting the vis parameter to True or False respectively. The following methods are available to turn on/off visualizations:\n",
      "\n",
      "* phenotype\\_measurement\\_pepper(img, patch, fname, fruitIdx, vis=False, ppx=None, ppy=None)\n",
      "* run\\_inference(det, learn, img, vis=False)\n",
      "\n",
      "You can call these methods with the vis parameter set to True or False to turn on/off visualizations.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How to turn on/off visulaizations of detections and phenotypes of hot peppers lsit all methods?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7c150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
