{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11cab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 23 00:08:19 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "|  0%   17C    P8              15W / 300W |      2MiB / 23028MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17412960",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa8efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets loralib sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a1d558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.1 which is incompatible.\r\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3227d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (4.37.0.dev0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (2.1.2)\n",
      "Collecting torchvision (from sentence_transformers)\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (0.20.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence_transformers) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision->sentence_transformers) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=a7521a2e754f9bf3cf1ca5b15e0988812d055754bb1b29f75db334bab0c67f34\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: torchvision, sentence_transformers\n",
      "Successfully installed sentence_transformers-2.2.2 torchvision-0.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca48c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.9.21-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (3.8.6)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.6.3)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (2023.10.0)\n",
      "Collecting httpx (from llama-index)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (1.22.4)\n",
      "Collecting openai>=1.1.0 (from llama-index)\n",
      "  Downloading openai-1.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (8.2.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index)\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.1.0->llama-index) (4.0.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index)\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.1.0->llama-index) (1.10.13)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.1.0->llama-index) (1.3.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->llama-index) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx->llama-index)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->llama-index) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index) (0.14.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.31.0->llama-index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json->llama-index) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index) (1.1.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\n",
      "Downloading llama_index-0.9.21-py3-none-any.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: httpcore, distro, deprecated, tiktoken, httpx, openai, llama-index\n",
      "Successfully installed deprecated-1.2.14 distro-1.8.0 httpcore-1.0.2 httpx-0.26.0 llama-index-0.9.21 openai-1.6.1 tiktoken-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "911f8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67a65732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94b2e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents = SimpleDirectoryReader(\"example_data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e61bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(file_path):\n",
    "    return {\"file_name\": file_path.split(\"/\")[-1]}\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"/home/ec2-user/SageMaker/VegRD_APD_Fruit_Phenotyping/scripts\",recursive=True,\n",
    "                              required_exts=[\".py\"],file_metadata=get_meta)\n",
    "\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "131b00e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9e29638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='8b22e4ba-bcbd-4929-940f-666d167efdbb', embedding=None, metadata={'file_path': '/home/ec2-user/SageMaker/VegRD_APD_Fruit_Phenotyping/scripts/Hot_Peppers.py', 'file_name': 'Hot_Peppers.py', 'file_type': 'text/x-python', 'file_size': 20914, 'creation_date': '2023-12-20', 'last_modified_date': '2023-12-20', 'last_accessed_date': '2023-12-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='d2623851cefba477369443c24ced9fac58318b3b800ab4757dfaeaacbab9a2f0', text='import os\\nimport cv2\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport boto3\\nimport warnings\\nfrom tqdm import tqdm\\nfrom statistics import median\\nfrom skimage.morphology import medial_axis\\nfrom scipy.interpolate import splprep, splev\\nfrom scipy.ndimage import distance_transform_edt\\nfrom skimage.morphology import skeletonize\\nfrom lib.pred_utils import *\\nfrom lib.phenome_utils import *\\nfrom colorlog import ColoredFormatter\\nfrom config import HP_config_defaults as config_defaults\\nimport math\\nimport numpy as np\\nimport logging\\nfrom lib.aggregate import aggregate_data\\nwarnings.filterwarnings(\"ignore\")\\n\\nfrom skimage.morphology import skeletonize\\nfrom dsepruning import skel_pruning_DSE\\nfrom scipy.ndimage import distance_transform_edt\\nfrom lib.curved_utils import *\\n\\nimport argparse\\nimport json\\n# Parse command-line arguments\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\\'--config\\', help=\\'Path to configuration file\\')\\nargs = parser.parse_args()\\n\\n# Config dynamodb\\ndynamodb = boto3.resource(\\'dynamodb\\')\\ntable = dynamodb.Table(\\'veg-vps-Image_Analyzer-Images\\')\\n\\ndef load_config():\\n    return config_defaults\\n\\ndef setup_logging():\\n    logger = logging.getLogger()\\n    logger.setLevel(logging.INFO)\\n    handler = logging.StreamHandler()\\n    handler.setLevel(logging.INFO)\\n    formatter = ColoredFormatter(\\n        \"%(log_color)s%(levelname)-8s%(reset)s %(log_color)s%(message)s%(reset)s\",\\n        datefmt=None,\\n        reset=True,\\n        log_colors={\\n            \\'DEBUG\\': \\'cyan\\',\\n            \\'INFO\\': \\'green\\',\\n            \\'WARNING\\': \\'yellow\\',\\n            \\'ERROR\\': \\'red\\',\\n            \\'CRITICAL\\': \\'red\\',\\n        }\\n    )\\n    handler.setFormatter(formatter)\\n    logger.addHandler(handler)\\n    return logger\\n\\n## Keeping it here instead of curved_utils because of pruning import problem\\ndef backbone(patch, fname=None):\\n    _, patch = cv2.threshold(patch, 10, 255, cv2.THRESH_BINARY)\\n    skeleton = skeletonize(patch.copy()//255)\\n    dist = distance_transform_edt(patch.copy(), return_indices=False, return_distances=True)\\n    thres=[10000,1000,100]\\n    for th in thres:\\n        pruned = skel_pruning_DSE(skeleton, dist, th).astype(np.uint8)*255\\n        # If the pruned is not completely zero break\\n        if len(np.nonzero(pruned)[0]):\\n            break  \\n    points=skeletonEndpoints(pruned.copy())\\n    cv2.circle(pruned,(points[0][1],points[0][0]),2,color=255)\\n    cv2.circle(pruned,(points[1][1],points[1][0]),2,color=255)\\n    extended= extendSkel(points,pruned.copy(),patch.copy()//255)\\n    return extended\\n\\n\\n\\ndef load_images(img_path):\\n    all_imgs = []\\n    fnames = []\\n    for file_index, filename in enumerate(os.listdir(img_path)):\\n        if filename.endswith((\".tar\", \".json\", \".ipynb_checkpoints\")):\\n            continue\\n        f_p = os.path.join(img_path, filename)\\n        fnames.append(filename)\\n        all_imgs.append(f_p)\\n    return all_imgs, fnames\\n\\ndef calculate_scaled_points(points, ppx=None, ppy=None):\\n    if ppx is not None and ppy is not None:\\n        return np.array(points) * np.array([ppx, ppy])\\n    return np.array(points)\\n\\n\\ndef eucledian_distance(points, ppx=None, ppy=None):\\n    scaled_points = calculate_scaled_points(points, ppx, ppy)\\n    differences = np.diff(scaled_points, axis=0)\\n    squared_differences = np.square(differences)\\n    squared_distances = np.sum(squared_differences, axis=1)\\n    length = np.sqrt(squared_distances).sum()\\n    return length\\n\\n\\ndef eucledian_list(inp, ppx=None, ppy=None):\\n    return [eucledian_distance(item, ppx, ppy) for item in inp]\\n\\n\\ndef convert_pixels_to_measure(params, ppx=None, ppy=None):\\n    \"\"\"Convert pixels to real units cm/mm if ppx and ppy are defined, otherwise find lengths of curves\"\"\"\\n    if ppx is not None and ppy is not None:\\n        params[\"area\"] *= ppx * ppy\\n        params[\"perimeter\"] *= ppx\\n    params[\"mid_height_width\"] = eucledian_distance(params[\"mid_height_width_p\"], ppx, ppy)\\n    params[\"max_width\"] = eucledian_distance(params[\"max_width_p\"], ppx, ppy)\\n    params[\"all_widths\"] = eucledian_list(params[\"all_widths_p\"], ppx, ppy)\\n    params[\"mid_width_height\"] = eucledian_distance(params[\"mid_width_height_p\"], ppx, ppy)\\n    params[\"max_height\"] = eucledian_distance(params[\"max_height_p\"], ppx, ppy)\\n    params[\"all_heights\"] = eucledian_list(params[\"all_heights_p\"], ppx, ppy)\\n    if isinstance(params[\"c_length\"],list) or isinstance(params[\"c_length\"],np.ndarray):\\n        params[\"c_length\"] = eucledian_distance(params[\"c_length\"], ppx, ppy)\\n    else:\\n        params[\"c_length\"] *= ppx\\n    return params\\n\\n\\ndef get_unique_xs(per_points):\\n    xs = per_points[:, 0]\\n    u_xs = sorted(set(xs))\\n    return u_xs, xs\\n\\n\\ndef process_segments(u_xs, xs, ys):\\n    top = []\\n    bottom = []\\n    widths = []\\n    for x in u_xs:\\n        segments = min_max_segment(x, xs, ys)\\n        for segment in segments:\\n            top.append(segment[0])\\n            bottom.append(segment[1])\\n            widths.append(segment[2])\\n    return top, bottom, widths\\n\\ndef calculate_shape_metrics(kwargs, ppx=None, ppy=None):\\n    max_height = kwargs[\"max_height\"]\\n    max_width = kwargs[\"max_width\"]\\n    mid_height_width = kwargs[\"mid_height_width\"]\\n    mid_width_height = kwargs[\"mid_width_height\"]\\n    c_length = kwargs[\"c_length\"]\\n    t_block = kwargs[\"t_block\"]\\n    b_block = kwargs[\"b_block\"]\\n    maxheight_to_maxwidth = max_height / max_width\\n    mid_width_height_to_mid_height_width = mid_width_height / mid_height_width\\n    c_length_to_mid_height_width = c_length / mid_height_width\\n    proximal_blockiness = eucledian_distance(t_block, ppx=ppx, ppy=ppy) / mid_height_width\\n    distal_blockiness = eucledian_distance(b_block, ppx=ppx, ppy=ppy) / mid_height_width\\n    fruit_triangle = eucledian_distance(t_block, ppx=ppx, ppy=ppy) / eucledian_distance(b_block, ppx=ppx, ppy=ppy)\\n\\n    return (maxheight_to_maxwidth, mid_width_height_to_mid_height_width, c_length_to_mid_height_width,\\n            proximal_blockiness, distal_blockiness, fruit_triangle)\\n\\n\\ndef phenotype_measurement_pepper(img, patch, fname, fruitIdx, corrupted_color_flag=False, skip_outliers=None, pct_width=0.4, vis=False,ppx=None,ppy=None):\\n    width_quantiles = config_defaults[\"width_quantiles\"]\\n    expected_sample_len = len(width_quantiles)\\n    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\\n    contours = max(contours, key=cv2.contourArea)\\n    per_points = contours.squeeze()\\n    per_points = np.array(sorted(per_points, key=lambda x: x[0]))\\n\\n    perimeter = cv2.arcLength(contours, True)\\n    area = cv2.contourArea(contours)\\n    area_points = np.where(img != 0)\\n    fruit_colors = get_color(patch.copy(), area_points)\\n    \\n    ys = per_points[:, 1]\\n    u_xs,xs = get_unique_xs(per_points)\\n\\n    max_height_p, max_height_v, all_heights_p, all_heights_v = find_max_height_revised(xs, ys)\\n    mid_width_height_p, mid_width_height_v = find_mid_width_height(xs, ys)\\n    \\n    \\n    extended=None\\n    try:\\n    \\n        extended=backbone(img)\\n\\n        ## Widths\\n        top, bottom, widths =get_curved_widths(extended,img)\\n\\n        max_width_p, max_midth_v, all_widths_points, all_widths_p = find_max_width(top, bottom, quantiles=width_quantiles, expected_sample_len=expected_sample_len)\\n\\n        mid_height_width_p, mid_height_width_v = find_mid_height_width(top, bottom)\\n\\n        # curve length\\n        #c_length = find_curve_length(top, bottom, img)\\n        c_length,c_length_p=get_curved_length(extended)\\n        b_block, m_block, t_block = blockiness(top, bottom)\\n        \\n    except Exception as inst:\\n        cv2.imwrite(f\"exception_img.png\",img)\\n        if extended is not None:\\n            cv2.imwrite(f\"Extended_backbone.png\",extended)\\n            \\n        return [None]*22\\n    \\n    \\n    b_block, m_block, t_block = blockiness(top, bottom)\\n\\n    ellipse, ellipse_err = ellipse_fitting_normalized(img)\\n    box, box_aspect = box_fit_normalized(img)\\n\\n    if vis:\\n        phen_img = vis_phenotypes(\\n            per_points,\\n            area_points,\\n            max_height_p,\\n            all_heights_p,\\n            mid_width_height_p,\\n            max_width_p,\\n            all_widths_p,\\n            mid_height_width_p,\\n            c_length_p,\\n            b_block,\\n            m_block,\\n            t_block,\\n            box,\\n            ellipse,\\n            patch.copy(),\\n        )\\n\\n        widths_img = vis_ends_widths(all_widths_p, patch.copy())\\n\\n    else:\\n        phen_img = None\\n        widths_img = None\\n    \\n    params = {\\n    \"area\": area,\\n    \"perimeter\": perimeter,\\n    \"mid_height_width_p\": mid_height_width_p,\\n    \"max_width_p\": max_width_p,\\n    \"all_widths_p\": all_widths_p,\\n    \"mid_width_height_p\": mid_width_height_p,\\n    \"max_height_p\": max_height_p,\\n    \"all_heights_p\": all_heights_p,\\n    \"c_length\": c_length}\\n    \\n    results=convert_pixels_to_measure(params, ppx, ppy)\\n    \\n    results[\"m_block\"]=m_block\\n    results[\"t_block\"]=t_block\\n    results[\"b_block\"]=b_block\\n    \\n    shape_metrics = calculate_shape_metrics(results,ppx=ppx,ppy=ppy)\\n    \\n    maxheight_to_maxwidth, mid_width_height_to_mid_height_width, c_length_to_mid_height_width,proximal_blockiness, distal_blockiness,fruit_triangle=shape_metrics\\n    \\n    return(\\n    results[\"area\"],\\n    results[\"perimeter\"],\\n    results[\"mid_height_width\"],\\n    results[\"max_width\"],\\n    results[\"all_widths\"],\\n    results[\"mid_width_height\"],\\n    results[\"max_height\"],\\n    results[\"all_heights\"],\\n    results[\"c_length\"],\\n    maxheight_to_maxwidth,\\n    mid_width_height_to_mid_height_width,\\n    c_length_to_mid_height_width,\\n    proximal_blockiness,distal_blockiness,fruit_triangle,\\n    ellipse_err,box_aspect,\\n    fruit_colors,\\n    phen_img,\\n    # Added for validation and debugging\\n    max_height_p,\\n    mid_height_width_p,\\n    widths_img)\\n    \\n\\ndef run_inference(det,learn,img):\\n    if img is not None:\\n        detected_cucumber,all_masks,all_patches,boxes,*_=det.pred(img)\\n    else:\\n        print(f\"Invalid Image skipping!!\")\\n        return\\n    cropedPatches=[]\\n    cropedmasks=[]\\n    croppedboxes = []\\n    for i,patch in enumerate(all_patches):\\n        x, y, w, h = cv2.boundingRect(cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY))\\n        newImg = patch[y:y+h, x:x+w]\\n        mask=all_masks[i][y:y+h, x:x+w]\\n        mask = np.where(mask < 50, 0, 255).astype(np.uint8)\\n        cropedPatches.append(newImg)\\n        cropedmasks.append(mask)\\n        croppedboxes.append((x, y, w, h))\\n        \\n    predPoints={}\\n    for i,crop in enumerate(cropedPatches):\\n        img = PILImage.create(crop[...,::-1])\\n        pred=learn.predict(img)\\n        preds=scale_predictions(pred,img.shape)\\n        predPoints[i]=[tuple(preds[0].round().long().numpy()),tuple(preds[1].round().long().numpy())]\\n    \\n    # Correction\\n    corrected_imgs=apply_rotation(cropedPatches,predPoints)\\n    corrected_masks=apply_rotation(cropedmasks,predPoints)\\n\\n\\n    # Use angle of major axis to make the final rotation\\n    ellipse_rotated_imgs = []\\n    ellipse_rotated_masks = []\\n    ellipse_rotated_angle = []\\n    for corrected_img, corrected_mask in zip(corrected_imgs, corrected_masks):\\n        (_,_,_,_,angle),_ = ellipse_fitting_normalized(corrected_mask)\\n        ellipse_rotated_imgs.append(rotate_image(corrected_img, angle-90))\\n        ellipse_rotated_masks.append(rotate_image(corrected_mask, angle-90))\\n        ellipse_rotated_angle.append(angle)\\n    \\n    corrected_imgs = ellipse_rotated_imgs\\n    corrected_masks = ellipse_rotated_masks\\n        \\n    return (\\n        detected_cucumber,\\n        corrected_imgs,\\n        corrected_masks,\\n        croppedboxes,\\n        cropedPatches,\\n        predPoints,\\n        #original boxes are returned to get Gt based on location of boxes\\n        boxes\\n    )\\n\\ndef get_weight(img_name,proj_dynamo):\\n    payload={\\'ProjName\\': proj_dynamo,\\'Name\\': img_name}\\n    response = table.get_item(Key=payload)\\n    try:\\n        weight=float(response[\\'Item\\'][\\'meta_data\\'][\\'weight\\'])\\n        assert weight is not None\\n        # for FTS, assign weight value to 0 if less than 1e-2\\n        if weight < 1e-2:\\n            weight = 0\\n    except Exception as e:\\n        logger.warn(f\"Exception is {e}\")\\n        logger.warn(f\"Response of error is {response} for image {img_name}\")\\n        weight=np.nan\\n    return weight\\n                                                                             \\n\\ndef add_or_append_to_dict(dictionary, key, value):\\n    if key not in dictionary:\\n        dictionary[key] = [value]\\n    else:\\n        dictionary[key].append(value)\\n\\n\\ndef main(config):\\n    logger = setup_logging()\\n    #config = load_config()\\n    # Access configuration values:\\n    imgs_path = config[\\'img_path\\']\\n    proj_dynamo = config[\\'proj_dynamo\\']\\n    save_path = config[\\'save_path\\']\\n    #indiv_results = config[\\'indiv_results\\']\\n    #aggregate_results = config[\\'aggregate_results\\']\\n    os.makedirs(save_path, exist_ok=True)\\n    width_quantiles = config[\\'width_quantiles\\']\\n    expected_sample_len = config[\\'expected_sample_len\\']\\n    ppx = config[\\'ppx\\']\\n    ppy = config[\\'ppy\\']\\n    warnings.filterwarnings(\"ignore\")\\n    # Define models\\' paths\\n    maskrcnn = config[\\'maskrcnn\\']\\n    maskrcnn_config = config[\\'maskrcnn_config\\']\\n    resnet34 = config[\\'resnet34\\']\\n    marketable = config[\\'marketable\\']\\n    plant_traits=config[\\'plant_traits\\']\\n    corrupt_color_check = config[\"filter_corrupt_color\"]\\n    \\n    all_imgs,fnames=load_images(imgs_path)\\n    \\n    # Initialize MASKRCNN\\n    det = detectroninference(maskrcnn_config, maskrcnn, config[\"classes\"], conf_th=config[\"conf_det\"])\\n    # Initialize Resnet34\\n    learn = fast_AILearner(resnet34)\\n    \\n    results={}\\n    globalIndex=0\\n    empty_images=0\\n    skip_fruits=0\\n    plot_names=[]\\n    os.makedirs(save_path,exist_ok=True)\\n    # Set visualization flags\\n    visualize_phenotype = config[\"vis_phen\"]\\n    visualize_detection = config[\"vis_det\"]\\n\\n    logger.info(f\"Visualize detection {visualize_detection},  visualize phenotypes {visualize_phenotype}\")\\n\\n    # Create directory if it does not exist\\n    os.makedirs(save_path, exist_ok=True)\\n    starttime=time.monotonic()\\n\\n    # Process each image\\n    for index,img_path in tqdm(enumerate(all_imgs), total=len(all_imgs)):\\n        \\n        imgStartTime=time.monotonic()\\n        img = cv2.imread(img_path)\\n        detected, corrected_imgs, corrected_masks, cropped_boxes, cropedPatches, predPoints, boxes = run_inference(det,learn,img)\\n        logger.info(f\"Processing file {img_path}\")\\n\\n        # Append plot names to count plot\\n        plot_name = os.path.basename(img_path).split(\"_\")[0]\\n        if plot_name not in plot_names:\\n            plot_names.append(plot_name)\\n        \\n        # do a corrupted color check before phenotyping individual fruits\\n        # set a flag to be used in phenotype_measurement_pepper()\\n        # Kyle C uses L channel > 120\\n        if corrupt_color_check:\\n            corrupted_color_flag = corrupted_color_check(cv2.cvtColor(img.copy(), cv2.COLOR_BGR2Lab)[:,:,0], 120)\\n        else:\\n            corrupted_color_flag=False\\n            \\n        if corrupted_color_flag:\\n            cv2.imwrite(os.path.join(\"wrong_color\",fnames[index]),img)\\n\\n        # Process each fruit in the image\\n        for j, mask in enumerate(corrected_masks):\\n            (\\n                area, perimeter, mid_height_width, max_width, all_widths, mid_width_height,\\n                max_height, all_heights, c_length, maxheight_to_maxwidth,\\n                mid_width_height_to_mid_height_width, c_length_to_mid_height_width,\\n                proximal_blockiness, distal_blockiness, fruit_triangle, ellipse_err,\\n                box_aspect, fruit_colors, phen_img, tmp_pred_ls_ps, tmp_pred_ws_ps, widths_img\\n            ) = phenotype_measurement_pepper (\\n                mask, corrected_imgs[j], os.path.basename(img_path), j,corrupted_color_flag,\\n                skip_outliers=True, vis=visualize_phenotype,ppx=ppx,ppy=ppy)\\n\\n            # If any phenotype is None continue\\n            if area is None:\\n                logger.error(f\"Cannot process this fruit!\")\\n                empty_images += 1\\n                continue\\n            else:\\n                globalIndex+=1\\n\\n            # Save phenotype image\\n            if visualize_phenotype and phen_img is not None:\\n                cv2.imwrite(os.path.join(save_path, f\"Phenotype_{j}_{os.path.basename(img_path)}\"), phen_img)\\n\\n            bb = boxes[j]\\n\\n            keys_and_values = [\\n            (\"Image\", fnames[index]),\\n            (\"Area\", area),\\n            (\"Perimeter\", perimeter),\\n            (\"Mid_Width\", mid_height_width),\\n            (\"Max_Width\", max_width),\\n            (\"FWSD\", np.std(all_widths)),\\n            (\"Mid_Height\", mid_width_height),\\n            (\"Max_Height\", max_height),\\n            (\"FHSD\", float(np.std(all_heights))),\\n            (\"Curved_Height\", c_length),\\n            (\"Maxheight_to_maxwidth\", maxheight_to_maxwidth),\\n            (\"Midheight_to_midwidth\", mid_width_height_to_mid_height_width),\\n            (\"Curveheight_to_midwidth\", c_length_to_mid_height_width),\\n            (\"Proximal_blockiness\", proximal_blockiness),\\n            (\"Distall_blockiness\", distal_blockiness),\\n            (\"Fruit_triangle\", fruit_triangle),\\n            (\"Ellipse\", ellipse_err),\\n            (\"Box\", box_aspect),\\n            (\"Box_x\", (bb[0] + bb[2]) // 2),\\n            (\"Box_y\", (bb[1] + bb[3]) // 2)]\\n\\n            for key, value in keys_and_values:\\n                add_or_append_to_dict(results, key, value)\\n\\n            for i, curr_width in enumerate(all_widths):\\n                add_or_append_to_dict(results, f\"FWID{(i+1):02}\", curr_width)\\n\\n            for i, curr_height in enumerate(all_heights):\\n                add_or_append_to_dict(results, f\"FHT{(i+1):02}\", curr_height)\\n\\n            for color_pheno, color_pheno_tup in config[\"color_pheno\"].items():\\n                if len(color_pheno_tup) == 0:\\n                    add_or_append_to_dict(results, color_pheno, fruit_colors[color_pheno])\\n                else:\\n                    for i, channel in enumerate(color_pheno_tup):\\n                        add_or_append_to_dict(results, f\"{color_pheno}_{channel}\", fruit_colors[color_pheno][i])\\n        imgEndTime=time.monotonic()\\n        logger.info(f\"Image processing time: {imgEndTime - imgStartTime:.2f} seconds\")\\n        # Save detection results\\n        if visualize_detection:\\n            save_results_cv(detected, cropedPatches, predPoints, corrected_imgs, os.path.basename(img_path), os.path.join(save_path, \"inter\"))\\n        #if globalIndex>20:\\n        #    break\\n            \\n    endtime=time.monotonic()\\n    # Print some statistics\\n    logger.warn(f\"Number of empty images: {empty_images}\")\\n    logger.info(f\"Number of images processed: {len(all_imgs) - empty_images}\")\\n    logger.info(f\"Number of unique plots: {len(plot_names)}\")\\n    logger.info(f\"Total processing time: {endtime - starttime:.2f} seconds\")\\n\\n    #with open(\\'results_refactor2.pkl\\', \\'wb\\') as f:\\n    #    pickle.dump(results, f)\\n\\n    pd.set_option(\"display.max_columns\", None)\\n    results_df=pd.DataFrame.from_dict(results,orient=\\'index\\').transpose()\\n    ## Explicitly specify the types of columns\\n    # separate column names by data type\\n    objcols = [\"Image\"]\\n    results_df[results_df.columns.difference(objcols)] = results_df[results_df.columns.difference(objcols)].astype(\\'float64\\')\\n    # Add PLOTBID and Date\\n    results_df.insert(loc=1, column=\\'plot_bid\\', value=results_df[\"Image\"].str.extract(r\\'(.*?)_\\'))\\n    # This is bug in automation code of scalecam but puting a fix here temporarily\\n    #results_df[\\'plot_bid\\'] = results_df[\\'plot_bid\\'].str.replace(r\\'^SHIFT\\', \\'\\')\\n    results_df[\\'plot_bid\\'] = results_df[\\'plot_bid\\'].str.replace(r\\'^.+(?<=SHIFT)\\', \\'\\', regex=True)\\n    results_df.insert(loc=2, column=\\'date\\', value=results_df[\"Image\"].str.extract(r\\'_(.*?)_\\'))\\n    # Sort by date\\n    results_df = results_df.sort_values(\"date\", ascending=True).reset_index(drop=True)\\n    \\n    if not os.path.exists(save_path):\\n        os.makedirs(save_path)\\n    \\n    results_df.to_csv(save_path+\"/indiv.csv\",index=False)\\n    ## Get weight data\\n    images=results_df[\"Image\"].tolist()\\n    weights={\"Image\":[],\"Weights\":[]}\\n    for img in images:\\n        weights[\"Image\"].append(img)\\n        logger.info(f\"Getting weight for image {img}\")\\n        weights[\"Weights\"].append(get_weight(img,proj_dynamo))\\n    weights_df=pd.DataFrame.from_dict(weights,orient=\\'index\\').transpose()\\n    \\n    aggregate_data(results_df, weights_df,imgs_path, save_path, output_filename=\"aggregate.csv\",logger=logger,marketable=marketable,plant_traits=plant_traits)\\n    logger.info(f\"Uploading results in s3 bucket\")\\n    if config[\\'save_aws\\']:\\n        save_aws(directory_path=save_path,bucket_name=\"470086202700-sagemaker\",projname=proj_dynamo)\\n\\nif __name__ == \"__main__\":\\n    from config import HP_config_defaults as config_defaults\\n    # Load configuration from file if provided, otherwise use default config\\n    if args.config:\\n        with open(args.config, \\'r\\') as f:\\n            cfg = json.load(f)\\n    else:\\n        cfg = config_defaults\\n    main(cfg)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72eb3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1d9c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = \"<|USER|>{query_str}<|ASSISTANT|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b36075",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HuggingFaceLLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# the device to load the model onto\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceLLM\u001b[49m(\n\u001b[1;32m      7\u001b[0m     context_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m      8\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      9\u001b[0m     generate_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[1;32m     10\u001b[0m     system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[1;32m     11\u001b[0m     query_wrapper_prompt\u001b[38;5;241m=\u001b[39mquery_wrapper_prompt,\n\u001b[1;32m     12\u001b[0m     tokenizer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     device_map\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     15\u001b[0m     tokenizer_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4096\u001b[39m},\n\u001b[1;32m     16\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat32},\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#llm = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#tokenizer.batch_decode(generated_ids)[0]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HuggingFaceLLM' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"auto\" # the device to load the model onto\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=1000,\n",
    "    generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map=device,\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": torch.float32},\n",
    "\n",
    ")\n",
    "     \n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "#prompt = \"My favourite condiment is\"\n",
    "\n",
    "#model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "#model.to(device)\n",
    "\n",
    "#generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "#tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c4f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embed_model =HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\",model_kwargs = {'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f738515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " 'aembed_documents',\n",
       " 'aembed_query',\n",
       " 'cache_folder',\n",
       " 'client',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'embed_documents',\n",
       " 'embed_query',\n",
       " 'encode_kwargs',\n",
       " 'from_orm',\n",
       " 'json',\n",
       " 'model_kwargs',\n",
       " 'model_name',\n",
       " 'multi_process',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de89fc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-base-en\n",
      "Load pretrained SentenceTransformer: BAAI/bge-base-en\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "embed_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en\",model_kwargs = {'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d592c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install -U angle-emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7808f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOTA Embeddings from huggingface leaderboard\n",
    "\n",
    "from typing import Any, List\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "from llama_index.bridge.pydantic import PrivateAttr\n",
    "\n",
    "## Embedding model specific current SOTA in huggung face leaderboard\n",
    "from angle_emb import AnglE, Prompts\n",
    "\n",
    "\n",
    "class UAEmbeddings(BaseEmbedding):\n",
    "    \n",
    "    _model: Any= PrivateAttr()\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        instructor_model_name: str = 'WhereIsAI/UAE-Large-V1',\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__(**kwargs)  \n",
    "        self._model=AnglE.from_pretrained(instructor_model_name, pooling_strategy='cls',)\n",
    "        #self._model.set_prompt(prompt=None)\n",
    "\n",
    "        \n",
    "    def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        # Implementation of the abstract method\n",
    "        embeddings = self._model.encode([ query])\n",
    "        return embeddings[0]\n",
    "        \n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        embeddings = self._model.encode([query])\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        embeddings = self._model.encode([text])\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self._model.encode(\n",
    "            [text for text in texts]\n",
    "        )\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68fb74a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model=UAEmbeddings()\n",
    "embed_model._model.device\n",
    "#embeddings = embed_model._get_text_embedding(\"It is raining cats and dogs here!\")\n",
    "#embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708bb22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: InstructorEmbedding in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install InstructorEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "from llama_index.bridge.pydantic import PrivateAttr\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "\n",
    "\n",
    "class InstructorEmbeddings(BaseEmbedding):\n",
    "    _model: INSTRUCTOR = PrivateAttr()\n",
    "    _instruction: str = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instructor_model_name: str = \"hkunlp/instructor-large\",\n",
    "        instruction: str = \"Represent code base to easily search from:\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self._model = INSTRUCTOR(instructor_model_name,device=\"cpu\")\n",
    "        self._instruction = instruction\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"instructor\"\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_text_embedding(text)\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        embeddings = self._model.encode([[self._instruction, query]])\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        embeddings = self._model.encode([[self._instruction, text]])\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self._model.encode(\n",
    "            [[self._instruction, text] for text in texts]\n",
    "        )\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model=InstructorEmbeddings()\n",
    "#embeddings = embed_model._get_text_embedding(\"It is raining cats and dogs here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d646532",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "252f4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91f0991a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb34dc2396444038277fbf70e3567f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f79198d43194e1490b3778f5d677e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453d6d7c8fbf4bc893cec8c8d97a4dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bc54edbb3b4f86a6ccb0bb501a9086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3e45c4df8d43dc8a89c7693bc39008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060012fa69be4226a8904417dd3c2009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6f2682c69349c18930a3bd6cb2c890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196897b1308e43718e4562b414367cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dee1e8121ce48ae866db0070750ce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da596b03888e41098ac6acdf3bd12dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133514757e174e6eaeac300c994bb624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf3b60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dca8205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3485a7b236954e4ea0e4e914fa9ef056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm to calculate c-length of hot peppers is as follows:\n",
      "\n",
      "1. Convert pixels to real units cm/mm if ppx and ppy are defined, otherwise find lengths of curves.\n",
      "2. Process the segments of the hot pepper by finding the top, bottom, and widths of each segment.\n",
      "3. Calculate the shape metrics of the hot pepper, including the c-length, using the processed segments.\n",
      "\n",
      "The c-length is calculated by dividing the length of the hot pepper by the width of the mid-height region.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what is algorithm to calculate c-length of hot peppers?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f023fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eeed8b298184ab7bd5094a2d9b39956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How to turn on/off visulaizations of phenotypes?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06a3345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can turn on/off visualizations of detections and phenotypes of hot peppers by setting the vis parameter to True or False respectively. The following methods are available to turn on/off visualizations:\n",
      "\n",
      "* phenotype\\_measurement\\_pepper(img, patch, fname, fruitIdx, vis=False, ppx=None, ppy=None)\n",
      "* run\\_inference(det, learn, img, vis=False)\n",
      "\n",
      "You can call these methods with the vis parameter set to True or False to turn on/off visualizations.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How to turn on/off visulaizations of detections and phenotypes of hot peppers lsit all methods?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7c150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
