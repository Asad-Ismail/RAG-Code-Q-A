{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17412960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b02fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a65732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from git import Repo\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b2e1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    \"/home/ec2-user/SageMaker/VegRD_APD_Fruit_Phenotyping/scripts\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=5000),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131b00e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e29638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='import numpy as np\\nimport os, sys\\nfrom collections import defaultdict\\nfrom get_data import get_images\\nimport re\\n\\ndef update_config(config_defaults,**kwargs):\\n    new_config = config_defaults.copy()\\n    for key, value in kwargs.items():\\n        if key in new_config:\\n            new_config[key] = value\\n        else:\\n            raise KeyError(f\"Unknown configuration key: {key}\")\\n    return new_config\\n\\n\\ndef pattern_exists_in_string(string, pattern=\"BLK\"):\\n    if re.search(pattern, string):\\n        return True\\n    else:\\n        return False\\n\\n\\n# A dictionary of projects and images\\nprojs = defaultdict(list)\\n\\nwith open(\"new_images.txt\", \"r\") as f:\\n    for l in f.readlines():\\n        l=l.strip()\\n        splitted= l.split(\"/\")\\n        projs[splitted[1]].append(l)\\n\\nfor k,v in projs.items():\\n    get_images(k,s3_bucket=\"model-deployment-automation\", files_to_copy=v)\\n    if pattern_exists_in_string(k,pattern=\"BLK\"):\\n        from config import blocky_config_defaults as config_defaults\\n        from blocky_peppers import *     \\n        new_config=update_config(config_defaults,img_path=os.path.join(\"filtered\",k),proj_dynamo=os.path.join(\"Pepper\",k),\\n                                save_path=os.path.join(\"results\",k),save_aws=True,aws_path=\"automated\")\\n        main(new_config)\\n    else:\\n       raise ValueError(f\"{k} models is not Implemented\")\\n', metadata={'source': '/home/ec2-user/SageMaker/VegRD_APD_Fruit_Phenotyping/scripts/automate.py', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c968aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=2000, chunk_overlap=200)\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5920c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd0728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "#embeddings = HuggingFaceInstructEmbeddings(\n",
    "#    query_instruction=\"Query about the code base: \",\n",
    "#    model_kwargs = {'device': 'cpu'}\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9fe6df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchain sentence_transformers\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6c54086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "#model_name = \"BAAI/BAAI/bge-base-en\"\n",
    "#model_kwargs = {\"device\": \"cpu\"}\n",
    "#encode_kwargs = {\"normalize_embeddings\": True}\n",
    "#hf = HuggingFaceBgeEmbeddings(\n",
    "#    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e77eb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_embeddings = [embeddings.embed_documents(text.page_content) for text in texts]\n",
    "from langchain.vectorstores import Chroma\n",
    "# Create a vector store\n",
    "#vector_store = Chroma.from_embeddings(document_embeddings)\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314cbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "#from langchain.vectorstores import Chroma\n",
    "\n",
    "#db = Chroma.from_documents(texts, embeddings)\n",
    "#retriever = db.as_retriever(\n",
    "#    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "#    search_kwargs={\"k\": 8},\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee070f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f6e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "#import torch\n",
    "\n",
    "#device=\"cpu\"\n",
    "\n",
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype= torch.float32,device_map=device)\n",
    "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1000,temperature=0.1, do_sample=True)\n",
    "#llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16238a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1f6f2247f14a7f80d05612e4dc4774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "device=\"cpu\"\n",
    "model_id = \"openchat/openchat-3.5-1210\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype= \"auto\",device_map=device,trust_remote_code=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1000)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25197616",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
    "\n",
    "#outputs = model.generate(**inputs, max_length=200)\n",
    "#text = tokenizer.batch_decode(outputs)[0]\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24abb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "gpu_chain = prompt | llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "#print(gpu_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fea2d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gpt4all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ec0e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import GPT4All\n",
    "\n",
    "#llm = GPT4All(\n",
    "#    model=\"gpt4all-falcon-q4_0.gguf\",\n",
    "#    max_tokens=2048,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11df6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the device and dtype\n",
    "# Get the first parameter of the model and check its device and dtype\n",
    "#first_param = next(model.parameters())\n",
    "#model_device = first_param.device\n",
    "#model_dtype = first_param.dtype\n",
    "\n",
    "#print(f\"Model is on device: {model_device}\")\n",
    "#print(f\"Model data type: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be9f6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test LLM without Context\n",
    "\n",
    "#from langchain.prompts import PromptTemplate\n",
    "\n",
    "#template = \"\"\"Question: {question}\n",
    "\n",
    "#Answer: Let's think step by step.\"\"\"\n",
    "#prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "#chain = prompt | llm\n",
    "\n",
    "#question = \"What is electroencephalography?\"\n",
    "\n",
    "#print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96fc3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=False)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=db.as_retriever(),memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db86289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0857641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"How is curved backbone of hot peppers calculated?\"\n",
    "#docs = db.similarity_search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a935d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75da2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain import hub\n",
    "\n",
    "#rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "#from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Chain\n",
    "#chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=rag_prompt)\n",
    "#chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2360b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d712403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/generation/utils.py:1547: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "question = \"How is curved backbone of hot peppers calculated?\"\n",
    "result = qa(question)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How is curved backbone of hot peppers calculated?\",\n",
    "    \"How to turn on/off visulaizations of phenotypes?\",\n",
    "    \"How to turn on/off visulaizations of detections and phenotypes of hot peppers lsit all methods?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa(question)\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b343f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
