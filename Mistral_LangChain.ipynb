{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17412960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b02fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a65732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from git import Repo\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b2e1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    \"/home/ec2-user/SageMaker/VegRD_APD_Fruit_Phenotyping/scripts\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=5000),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "131b00e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e29638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='import numpy as np\\nimport os, sys\\nfrom collections import defaultdict\\nfrom get_data import get_images\\nimport re\\n\\ndef update_config(config_defaults,**kwargs):\\n    new_config = config_defaults.copy()\\n    for key, value in kwargs.items():\\n        if key in new_config:\\n            new_config[key] = value\\n        else:\\n            raise KeyError(f\"Unknown configuration key: {key}\")\\n    return new_config\\n\\n\\ndef pattern_exists_in_string(string, pattern=\"BLK\"):\\n    if re.search(pattern, string):\\n        return True\\n    else:\\n        return False\\n\\n\\n# A dictionary of projects and images\\nprojs = defaultdict(list)\\n\\nwith open(\"new_images.txt\", \"r\") as f:\\n    for l in f.readlines():\\n        l=l.strip()\\n        splitted= l.split(\"/\")\\n        projs[splitted[1]].append(l)\\n\\nfor k,v in projs.items():\\n    get_images(k,s3_bucket=\"model-deployment-automation\", files_to_copy=v)\\n    if pattern_exists_in_string(k,pattern=\"BLK\"):\\n        from config import blocky_config_defaults as config_defaults\\n        from blocky_peppers import *     \\n        new_config=update_config(config_defaults,img_path=os.path.join(\"filtered\",k),proj_dynamo=os.path.join(\"Pepper\",k),\\n                                save_path=os.path.join(\"results\",k),save_aws=True,aws_path=\"automated\")\\n        main(new_config)\\n    else:\\n       raise ValueError(f\"{k} models is not Implemented\")\\n', metadata={'source': '/home/ec2-user/SageMaker/VegRD_APD_Fruit_Phenotyping/scripts/automate.py', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c968aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=2000, chunk_overlap=200)\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c5920c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd0728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "#embeddings = HuggingFaceInstructEmbeddings(\n",
    "#    query_instruction=\"Query about the code base: \",\n",
    "#    model_kwargs = {'device': 'cpu'}\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcf27143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchain sentence_transformers\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2110d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "#model_name = \"BAAI/BAAI/bge-base-en\"\n",
    "#model_kwargs = {\"device\": \"cpu\"}\n",
    "#encode_kwargs = {\"normalize_embeddings\": True}\n",
    "#hf = HuggingFaceBgeEmbeddings(\n",
    "#    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18fb1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_embeddings = [embeddings.embed_documents(text.page_content) for text in texts]\n",
    "from langchain.vectorstores import Chroma\n",
    "# Create a vector store\n",
    "#vector_store = Chroma.from_embeddings(document_embeddings)\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "314cbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "#from langchain.vectorstores import Chroma\n",
    "\n",
    "#db = Chroma.from_documents(texts, embeddings)\n",
    "#retriever = db.as_retriever(\n",
    "#    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "#    search_kwargs={\"k\": 8},\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee070f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2f6e032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e25f6217e6496496a1273497fffb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "device=\"cpu\"\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype= torch.float32,device_map=device)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1000,temperature=0.1, do_sample=True)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the device and dtype\n",
    "# Get the first parameter of the model and check its device and dtype\n",
    "first_param = next(model.parameters())\n",
    "model_device = first_param.device\n",
    "model_dtype = first_param.dtype\n",
    "\n",
    "print(f\"Model is on device: {model_device}\")\n",
    "print(f\"Model data type: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test LLM without Context\n",
    "\n",
    "#from langchain.prompts import PromptTemplate\n",
    "\n",
    "#template = \"\"\"Question: {question}\n",
    "\n",
    "#Answer: Let's think step by step.\"\"\"\n",
    "#prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "#chain = prompt | llm\n",
    "\n",
    "#question = \"What is electroencephalography?\"\n",
    "\n",
    "#print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=False)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=db.as_retriever(),memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b9d0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.14-py3-none-any.whl.metadata (478 bytes)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchainhub) (2.31.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.31.0.10-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (2023.7.22)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchainhub)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
      "Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: urllib3, types-requests, langchainhub\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.34.0 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.1.0 which is incompatible.\n",
      "kubernetes 28.1.0 requires urllib3<2.0,>=1.24.2, but you have urllib3 2.1.0 which is incompatible.\n",
      "sagemaker 2.200.0 requires urllib3<1.27, but you have urllib3 2.1.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.1 which is incompatible.\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchainhub-0.1.14 types-requests-2.31.0.10 urllib3-2.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c86d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How is curved backbone of hot peppers calculated?\"\n",
    "docs = db.similarity_search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c589ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=rag_prompt)\n",
    "chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a98d101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d712403",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How is curved backbone of hot peppers calculated??\"\n",
    "result = qa(question)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How is curved backbone of hot peppers calculated?\",\n",
    "    \"How to turn on/off visulaizations of phenotypes?\",\n",
    "    \"How to turn on/off visulaizations of detections and phenotypes of hot peppers lsit all methods?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa(question)\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b343f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
